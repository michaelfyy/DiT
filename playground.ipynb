{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ff4ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelfyy/miniconda3/envs/DiT/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from timm.models.vision_transformer import PatchEmbed\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0d152e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 3\n",
    "hidden_dim = 4\n",
    "\n",
    "pe = torch.Tensor(sequence_length, hidden_dim)\n",
    "for i in range(hidden_dim):\n",
    "    if (i % 2 == 0):\n",
    "        pe[:,i] = torch.sin(torch.arange(0, sequence_length) / (10000**(i/hidden_dim)))\n",
    "    else:\n",
    "        pe[:,i] = torch.cos(torch.arange(0, sequence_length) / (10000**((i-1)/hidden_dim)))\n",
    "\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9adee14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        p=4, \n",
    "        d=768,\n",
    "        img_size=32,\n",
    "        in_chans=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.img_size = img_size\n",
    "        self.in_chans = in_chans\n",
    "\n",
    "    def get_positional_embedding(self, x): # x.shape = torch.Size[(B, T, D)]\n",
    "        sequence_length = x.shape[-2]\n",
    "        hidden_dim = x.shape[-1]\n",
    "\n",
    "        positional_embedding = torch.Tensor(sequence_length, hidden_dim)\n",
    "        for i in range(hidden_dim):\n",
    "            if (i % 2 == 0):\n",
    "                positional_embedding[:,i] = torch.sin(torch.arange(0, sequence_length) / (10000**(i/hidden_dim)))\n",
    "            else:\n",
    "                positional_embedding[:,i] = torch.cos(torch.arange(0, sequence_length) / (10000**((i-1)/hidden_dim)))\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "    def forward(self, x): # x.shape = torch.Size[(B, 4, 32, 32)] (B, C, H, W)\n",
    "\n",
    "        # patch embed\n",
    "        patch_embedder = PatchEmbed(img_size=self.img_size, patch_size=self.p, in_chans=self.in_chans, embed_dim=self.d)\n",
    "        x = patch_embedder(x)\n",
    "\n",
    "        # add positional encoding\n",
    "        positional_embedding = self.get_positional_embedding(x)\n",
    "        x += positional_embedding\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Time_Embedder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim=768,\n",
    "            frequency=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.frequency = frequency\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def timestep_embedding(self, t, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                          These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an (N, D) Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.timestep_embedding(x, self.frequency)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Label_Embedder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=1000,\n",
    "            embedding_dim=768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.num_clases = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "class DiT_Block_Base(nn.Module): # like in-context but no conditioning\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=768,\n",
    "        num_heads=12\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.mha = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
    "\n",
    "        self.pointwise_mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "          \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        attn_output, attn_weights = self.mha(x, x, x)\n",
    "        x = residual + attn_output\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.pointwise_mlp(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "class DiT_Base(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim=768,\n",
    "            num_heads=12,\n",
    "            patch_size=4,\n",
    "            n_blocks=28,\n",
    "            num_classes=1000\n",
    "    ):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.patchifier = Patchify(p=self.patch_size, d=embedding_dim)\n",
    "        self.time_embedder = Time_Embedder(embedding_dim=embedding_dim)\n",
    "        self.label_embedder = Label_Embedder(embedding_dim=embedding_dim)\n",
    "        self.dit_blocks = nn.ModuleList([\n",
    "            DiT_Block_Base(embedding_dim=embedding_dim, num_heads=num_heads) for _ in n_blocks\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, time, label): # x.shape = torch.Size[(B, 4, 32, 32)] (B, C, H, W)\n",
    "        patch_embedding = self.patchifier(x)\n",
    "        time_embedding = self.time_embedder(time)\n",
    "        label_embedding = self.label_embedder(label)\n",
    "\n",
    "        tokens = torch.cat([patch_embedding, time_embedding, label_embedding], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bfe12351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_embedder = Time_Embedder(embedding_dim=768, frequency=256)\n",
    "y_embedder = Label_Embedder()\n",
    "patch_embedder = Patchify()\n",
    "\n",
    "t = torch.Tensor([0, 1, 2, 3, 4])\n",
    "t_embedding = t_embedder(t)\n",
    "\n",
    "label = torch.tensor([0, 0, 0, 0, 0])\n",
    "y_embedding = y_embedder(label)\n",
    "\n",
    "z = torch.rand(5, 4, 32, 32)\n",
    "z_embedding = patch_embedder(z)\n",
    "\n",
    "tokens = torch.cat([z_embedding, t_embedding.unsqueeze(1), y_embedding.unsqueeze(1)], dim=1)\n",
    "\n",
    "block = DiT_Block_Base()\n",
    "\n",
    "block(tokens).shape\n",
    "\n",
    "t_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c340dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokens[:, :-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3a5548d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 32, 32])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(768, 4*4*2*4)\n",
    "linear_out = linear(output)\n",
    "linear_out.shape\n",
    "\n",
    "final_out = rearrange(linear_out, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', p1=4, p2=4, h=32//4, w=32//4)\n",
    "final_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d10cc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from timm.models.vision_transformer import PatchEmbed\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Patchify(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        p=4, \n",
    "        d=768,\n",
    "        img_size=32,\n",
    "        in_chans=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.img_size = img_size\n",
    "        self.in_chans = in_chans\n",
    "\n",
    "    def get_positional_embedding(self, x): # x.shape = torch.Size[(B, T, D)]\n",
    "        sequence_length = x.shape[-2]\n",
    "        hidden_dim = x.shape[-1]\n",
    "\n",
    "        positional_embedding = torch.Tensor(sequence_length, hidden_dim)\n",
    "        for i in range(hidden_dim):\n",
    "            if (i % 2 == 0):\n",
    "                positional_embedding[:,i] = torch.sin(torch.arange(0, sequence_length) / (10000**(i/hidden_dim)))\n",
    "            else:\n",
    "                positional_embedding[:,i] = torch.cos(torch.arange(0, sequence_length) / (10000**((i-1)/hidden_dim)))\n",
    "\n",
    "        return positional_embedding\n",
    "\n",
    "    def forward(self, x): # x.shape = torch.Size[(B, 4, 32, 32)] (B, C, H, W)\n",
    "\n",
    "        # patch embed\n",
    "        patch_embedder = PatchEmbed(img_size=self.img_size, patch_size=self.p, in_chans=self.in_chans, embed_dim=self.d)\n",
    "        x = patch_embedder(x)\n",
    "\n",
    "        # add positional encoding\n",
    "        positional_embedding = self.get_positional_embedding(x)\n",
    "        x += positional_embedding\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Time_Embedder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim=768,\n",
    "            frequency=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.frequency = frequency\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency, embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def timestep_embedding(self, t, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                          These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an (N, D) Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.timestep_embedding(x, self.frequency)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Label_Embedder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=1000,\n",
    "            embedding_dim=768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.num_clases = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "class DiT_Block_Base(nn.Module): # like in-context but no conditioning\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=768,\n",
    "        num_heads=12\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.mha = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
    "\n",
    "        self.pointwise_mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "          \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        attn_output, attn_weights = self.mha(x, x, x)\n",
    "        x = residual + attn_output\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.pointwise_mlp(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim=768,\n",
    "            patch_size=4,\n",
    "            img_size=32,\n",
    "            num_channels=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, patch_size*patch_size*2*num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', \n",
    "                      p1=self.patch_size, p2=self.patch_size, h=self.img_size // self.patch_size, w=self.img_size // self.patch_size)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DiT_Base(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim=768,\n",
    "            num_heads=12,\n",
    "            patch_size=4,\n",
    "            n_blocks=28,\n",
    "            num_classes=1000,\n",
    "            frequency=256,\n",
    "            img_size=32,\n",
    "            in_chans=4,\n",
    "    ):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.in_chans = in_chans\n",
    "\n",
    "        self.patchifier = Patchify(p=self.patch_size, d=embedding_dim, img_size=img_size, in_chans=in_chans)\n",
    "        self.time_embedder = Time_Embedder(embedding_dim=embedding_dim, frequency=frequency)\n",
    "        self.label_embedder = Label_Embedder(num_classes=num_classes, embedding_dim=embedding_dim)\n",
    "        self.dit_blocks = nn.ModuleList([\n",
    "            DiT_Block_Base(embedding_dim=embedding_dim, num_heads=num_heads) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.final_layer = FinalLayer(embedding_dim=embedding_dim, patch_size=patch_size, img_size=img_size, num_channels=in_chans)\n",
    "        \n",
    "\n",
    "    def forward(self, x, time, label): # x.shape = torch.Size[(B, 4, 32, 32)] (B, C, H, W)\n",
    "        patch_embedding = self.patchifier(x)\n",
    "        time_embedding = self.time_embedder(time)\n",
    "        label_embedding = self.label_embedder(label)\n",
    "\n",
    "        tokens = torch.cat([patch_embedding, time_embedding.unsqueeze(1), label_embedding.unsqueeze(1)], dim=1) # add conditioning tokens\n",
    "        for dit_block in self.dit_blocks:\n",
    "            tokens = dit_block(tokens)\n",
    "\n",
    "        tokens = tokens[:, :-2, :] # remove conditioning tokens\n",
    "        tokens = self.final_layer(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6f7adf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 32, 32])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 16\n",
    "z = torch.rand(B, 4, 32, 32)\n",
    "t = torch.randint(64, (B,))\n",
    "y = torch.randint(1000, (B,))\n",
    "model = DiT_Base()\n",
    "\n",
    "model(z, t, y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6d98b607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 768])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 16\n",
    "D = 768\n",
    "\n",
    "z = torch.rand(B, 4, 32, 32)\n",
    "t = torch.randint(64, (B,))\n",
    "y = torch.randint(1000, (B,))\n",
    "\n",
    "patch_embedder = Patchify()\n",
    "z = patch_embedder(z) # (B, T, D)\n",
    "\n",
    "gamma1 = torch.rand(B, 1, D)\n",
    "beta1 = torch.rand(B, 1, D)\n",
    "\n",
    "out = gamma1 * z + beta1\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15531d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models import DiT_models\n",
    "\n",
    "model_name = 'DiT-XL/2'\n",
    "model = DiT_models[model_name](img_size=32, num_classes=1000)\n",
    "\n",
    "B = 16\n",
    "z = torch.rand(B, 4, 32, 32)\n",
    "t = torch.randint(64, (B,))\n",
    "y = torch.randint(1000, (B,))\n",
    "\n",
    "output = model(z, t, y)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9bd09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
